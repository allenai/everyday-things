{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4671c692",
   "metadata": {},
   "source": [
    "Other analysis such as majority class baseline.\n",
    "\n",
    "We also looked at relations overlapped across workers in our dataset to analyze if workers pay attention to similar or different aspects of everyday things. This is described in the \"Unanimity and diversity in parts mental models\" section of our Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7ec84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b27d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Class: 58.82 True: 6894, False: 4826\n"
     ]
    }
   ],
   "source": [
    "et2tripletslist = {}\n",
    "true_cnt = 0\n",
    "false_cnt = 0\n",
    "with open(\"enriched_mms/full-ET-dataset.tsv\", \"r\") as dataset:\n",
    "    lines = csv.reader(dataset, delimiter = \"\\t\")\n",
    "    for line_idx, line in enumerate(lines):\n",
    "        # skip header\n",
    "        if line_idx == 0:\n",
    "            continue\n",
    "            \n",
    "        et, turker = line[0], line[1]\n",
    "        if et not in et2tripletslist:\n",
    "            et2tripletslist[et] = {}\n",
    "            \n",
    "        # per MM as in an everyday thing sketched by a turker\n",
    "        et_turker = (et, turker)\n",
    "        if et_turker not in et2tripletslist[et]:\n",
    "            et2tripletslist[et][et_turker] = []\n",
    "           \n",
    "        # collect list of (triplet_tuple, True_False_label)\n",
    "        triplet = ast.literal_eval(line[2])\n",
    "        annotated_relation = (triplet, line[3])\n",
    "        if line[3] == \"True\":\n",
    "            true_cnt += 1\n",
    "        else:\n",
    "            false_cnt += 1\n",
    "        assert annotated_relation not in et2tripletslist[et][et_turker]\n",
    "        et2tripletslist[et][et_turker].append(annotated_relation)\n",
    "        \n",
    "print(\"Majority Class:\", round(max(true_cnt, false_cnt)/(true_cnt+false_cnt) * 100,2) , \"True: {}, False: {}\".format(true_cnt, false_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa004b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Class: 60.24 True: 306, False: 202\n"
     ]
    }
   ],
   "source": [
    "true_cnt = 0\n",
    "false_cnt = 0\n",
    "et_all_intersect_relations = {}\n",
    "for everyday_thing in et2tripletslist:\n",
    "    list_of_3mm = []\n",
    "    for mm_by_turker in et2tripletslist[everyday_thing]:\n",
    "        mm_relations = et2tripletslist[everyday_thing][mm_by_turker]\n",
    "        list_of_3mm.append(mm_relations)\n",
    "    assert len(list_of_3mm) == 3\n",
    "    \n",
    "    # converting the arrays into sets\n",
    "    s1 = set(list_of_3mm[0])\n",
    "    s2 = set(list_of_3mm[1])\n",
    "    s3 = set(list_of_3mm[2])\n",
    "    \n",
    "    # calculates intersection\n",
    "    set1 = s1.intersection(s2)    \n",
    "    result_set = set1.intersection(s3)\n",
    "        \n",
    "    # convert resulting set to list\n",
    "    final_list = list(result_set)\n",
    "    for _, label in final_list:\n",
    "        if label == \"True\":\n",
    "            true_cnt += 1\n",
    "        else:\n",
    "            false_cnt += 1\n",
    "\n",
    "    et_all_intersect_relations[everyday_thing] = final_list\n",
    "    \n",
    "print(\"Majority Class:\", round(max(true_cnt, false_cnt)/(true_cnt+false_cnt) * 100,2) , \"True: {}, False: {}\".format(true_cnt, false_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785b6d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# relations in overlap set: 508\n"
     ]
    }
   ],
   "source": [
    "et_cnt = [(entry, len(et_all_intersect_relations[entry])) for entry in et_all_intersect_relations]\n",
    "assert len(et_cnt) == 100\n",
    "cnt_et_dict = {}\n",
    "total_cnt = 0\n",
    "for et, cnt in et_cnt:\n",
    "    if cnt in cnt_et_dict:\n",
    "        cnt_et_dict[cnt].append(et)\n",
    "    else:\n",
    "        cnt_et_dict[cnt] = [et]\n",
    "    total_cnt += cnt\n",
    "#cnt_et_dict\n",
    "print(\"# relations in overlap set:\", total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1366bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 & coffee maker, fish\n",
      "28 & rabbit\n",
      "18 & deer\n",
      "16 & egg, electric stove, tree\n",
      "14 & ink pen\n",
      "12 & laptop, sandwich, rice cooker, airplane, table\n",
      "10 & fire extinguisher, bird\n",
      "8 & elevator, flashlight, stroller, dishwasher, kayak, ship, teapot, telescope, corn, hot air balloon, microwave\n",
      "6 & wheelchair, barbeque grill, kite, microphone, computer, duck, helicopter\n",
      "4 & pillow, truck, washing machine, door, hair dryer, rocket, screw, toaster, butterfly, chair, knife, photo frame, shoe, baby bottle, bed, bird cage, car, chainsaw, electric tea kettle, humidifier, piano\n",
      "2 & binoculars, digital camera, zipper, apple, digital clinical thermometer, earphone, flower, windmill, backpack, dog, doorbell, lightbulb, bat, cat, umbrella, stethoscope, tent\n",
      "0 & air conditioner, bicycle, blender, boat, glider, guitar, house, pencil sharpener, table fan, dryer, pencil, suitcase, telephone, microscope, refrigerator, space heater, typewriter, violin, wall clock, window, bookcase, bus, cable car, calculator, saucepan, train, cow, rat, table lamp\n"
     ]
    }
   ],
   "source": [
    "for cnt in sorted(cnt_et_dict, key=lambda x: x, reverse=True):\n",
    "    print(cnt, \"&\" ,\", \".join(cnt_et_dict[cnt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c668f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"Macaw-11B\"\n",
    "# model_name = \"Macaw-3B\"\n",
    "# model_name = \"Macaw-large\"\n",
    "# model_name = \"UnifiedQA\"\n",
    "model_name = \"gpt3-text-davinci-003\"\n",
    "\n",
    "statements_dir = \"0_\" + model_name + \"-ImagineADevice-CSP-Viz-full-ET-dataset/Props/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d450ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_for_given_belief_dict(to_evaluate, annotated_answers, verbose=False):\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    for triplet, gold_answer in annotated_answers:\n",
    "        if (gold_answer == \"True\" and triplet in to_evaluate) or (gold_answer == \"False\" and triplet not in to_evaluate):\n",
    "            correct += 1\n",
    "            print(\"MODEL CORRECT: relation\", triplet, \" gold answer:\", gold_answer)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"MODEL INCORRECT: relation\", triplet, \" gold answer:\", gold_answer)\n",
    "\n",
    "        total += 1\n",
    "        \n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04fb4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "before_name = \"model_believe_true_props\"\n",
    "after_name = \"maxsat_selected_props\"\n",
    "# before_name = \"model_believe_true_props_filtered\"\n",
    "# after_name = \"maxsat_selected_props_filtered\"\n",
    "improve_cnt = 0\n",
    "worsen_cnt = 0\n",
    "same_cnt = 0\n",
    "mm_total = 0\n",
    "overall_correct_cnt = {before_name:0, after_name: 0}\n",
    "overall_total_cnt = {before_name:0, after_name: 0}\n",
    "overall_improvement = 0\n",
    "\n",
    "acc_at_s = {before_name: {50: 0, 60: 0, 70: 0, 80: 0, 90: 0, 100: 0}, after_name: {50: 0, 60: 0, 70: 0, 80: 0, 90: 0, 100: 0}}\n",
    "\n",
    "# analysis\n",
    "size_of_mm_and_improvement = []\n",
    "\n",
    "for all_results_filename in os.listdir(statements_dir):\n",
    "    if all_results_filename.startswith(\".\"):\n",
    "        continue\n",
    "        \n",
    "    # Annotated answers\n",
    "    et, turker = all_results_filename.replace(\"_threshold50.pkl\", \"\").rsplit(\"_\",1)\n",
    "    et = et.replace(\"-\",\" \")\n",
    "    annotated_answers = et_all_intersect_relations[et]\n",
    "    print(et, turker)\n",
    "    if len(annotated_answers) == 0:\n",
    "        print(et, turker)\n",
    "        continue\n",
    "    \n",
    "    # Model's MM\n",
    "    with open(statements_dir + all_results_filename, 'rb') as f:\n",
    "            all_result_dict = pickle.load(f) \n",
    "    \n",
    "    # Evaluate\n",
    "    correct_cnt = {before_name:0, after_name: 0}\n",
    "    total_cnt = {before_name:0, after_name: 0}\n",
    "    \n",
    "    for prop_type in [before_name, after_name]:\n",
    "        to_evaluate = all_result_dict[prop_type]\n",
    "        print(prop_type)\n",
    "        correct, total = evaluate_accuracy_for_given_belief_dict(to_evaluate, annotated_answers, True)\n",
    "        correct_cnt[prop_type] = correct\n",
    "        overall_correct_cnt[prop_type] += correct\n",
    "        total_cnt[prop_type] = total\n",
    "        overall_total_cnt[prop_type] += total\n",
    "        \n",
    "        # Calculate accuracy at s\n",
    "        cur_acc = correct/total * 100\n",
    "        for s in acc_at_s[before_name]:\n",
    "            if cur_acc >= s:\n",
    "                acc_at_s[prop_type][s] += 1\n",
    "#         if prop_type == before_name and cur_acc < 20:\n",
    "#             print(all_results_filename)\n",
    "                \n",
    "        \n",
    "    assert total_cnt[before_name] == total_cnt[after_name]\n",
    "    print(\"model_believe\", \"accuracy: {}/{} ({})\".format(correct_cnt[before_name],\\\n",
    "            total_cnt[before_name],\\\n",
    "            round(correct_cnt[before_name]/total_cnt[before_name],2)))\n",
    "\n",
    "    print(\"maxsat_selected\", \"accuracy: {}/{} ({})\".format(correct_cnt[after_name],\\\n",
    "            total_cnt[after_name],\\\n",
    "            round(correct_cnt[after_name]/total_cnt[after_name],2)))\n",
    "    \n",
    "    improvement_w_maxsat = correct_cnt[after_name] - correct_cnt[before_name]\n",
    "    overall_improvement += improvement_w_maxsat\n",
    "    print(\"IMPROVEMENT W MAXSAT\", \"{}/{} ({})\".format(improvement_w_maxsat, total_cnt[after_name],\\\n",
    "        round(improvement_w_maxsat/total_cnt[after_name],2)))\n",
    "    if \"gpt3\" in model_name:\n",
    "        # (num_props, improvement, before_maxsat)\n",
    "        size_of_mm_and_improvement.append((len(all_result_dict['gpt3_predictions']), round(improvement_w_maxsat/total_cnt[after_name],2), round(correct_cnt[before_name]/total_cnt[before_name],2)))\n",
    "    else:\n",
    "        size_of_mm_and_improvement.append((len(all_result_dict['macaw_predictions']), round(improvement_w_maxsat/total_cnt[after_name],2), round(correct_cnt[before_name]/total_cnt[before_name],2)))\n",
    "    if improvement_w_maxsat == 0:\n",
    "        same_cnt += 1\n",
    "    elif improvement_w_maxsat > 0:\n",
    "        improve_cnt += 1\n",
    "    else:\n",
    "        worsen_cnt += 1\n",
    "    mm_total +=1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "285398f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full-ET-dataset-' + model_name + '-accuracy-dump-all-verbose-intersectMM.txt', 'w') as f:\n",
    "    f.write(cap.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea7366b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** gpt3-text-davinci-003 **********\n",
      "model_believe_true_props maxsat_selected_props\n",
      "model_believe_true_props\n",
      "acc at 50 : 165\n",
      "acc at 60 : 78\n",
      "acc at 70 : 66\n",
      "acc at 80 : 21\n",
      "acc at 90 : 21\n",
      "acc at 100 : 18\n",
      "maxsat_selected_props\n",
      "acc at 50 : 165\n",
      "acc at 60 : 154\n",
      "acc at 70 : 137\n",
      "acc at 80 : 131\n",
      "acc at 90 : 131\n",
      "acc at 100 : 131\n",
      "# MMs total: 213 improved: 140 worsen: 23 same: 50\n",
      "per query overall model_believe accuracy: 846/1524 (55.51)\n",
      "per query overall maxsat_selected accuracy: 1084/1524 (71.13)\n",
      "per query IMPROVEMENT W MAXSAT 238/1524 (15.62)\n"
     ]
    }
   ],
   "source": [
    "print(\"*\" * 10, model_name, \"*\" * 10)\n",
    "print(before_name, after_name)\n",
    "for before_after in acc_at_s:\n",
    "    print(before_after)\n",
    "    for s in acc_at_s[before_after]:\n",
    "        print(\"acc at\", s, \":\", acc_at_s[before_after][s])\n",
    "print(\"# MMs total:\", mm_total, \"improved:\", improve_cnt, \"worsen:\", worsen_cnt, \"same:\", same_cnt)\n",
    "print(\"per query overall model_believe\", \"accuracy: {}/{} ({})\".format(overall_correct_cnt[before_name],\\\n",
    "        overall_total_cnt[before_name],\\\n",
    "        round(overall_correct_cnt[before_name]/overall_total_cnt[before_name] * 100,2)))\n",
    "print(\"per query overall maxsat_selected\", \"accuracy: {}/{} ({})\".format(overall_correct_cnt[after_name],\\\n",
    "        overall_total_cnt[after_name],\\\n",
    "        round(overall_correct_cnt[after_name]/overall_total_cnt[after_name] * 100,2)))\n",
    "print(\"per query IMPROVEMENT W MAXSAT\", \"{}/{} ({})\".format(overall_improvement, overall_total_cnt[after_name],\\\n",
    "    round(overall_improvement/overall_total_cnt[after_name] * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c446165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everyday_devices",
   "language": "python",
   "name": "everyday_devices"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
