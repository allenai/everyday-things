{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdcf684",
   "metadata": {},
   "source": [
    "This notebook illustrates how we query GPT3 on everyday things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cbadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafe5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_chain, wait_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbcd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ba8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir gpt3_query_api_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245c670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following function taken from Niket's defeasible inference repo\n",
    "# https://github.com/allenai/defeasible_explanations/blob/fcb4a8dce3be2cb90511121f0a476b5b638d3f7c/src/prompting_utils.py\n",
    "def boolean_label(x):\n",
    "    true_indicators = {\"true\", \"t\", \"yes\", \"correct\", \"1\"}\n",
    "    false_indicators = {\"false\", \"f\", \"no\", \"wrong\", \"incorrect\", \"0\"}\n",
    "    if x.strip().lower() in true_indicators:\n",
    "        return True\n",
    "    elif x.strip().lower() in false_indicators:\n",
    "        return False\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18510b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_chain(*[wait_fixed(3) for i in range(3)] +\n",
    "                       [wait_fixed(5) for i in range(2)] +\n",
    "                       [wait_fixed(10)]))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42fae2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_false_prob_from_gpt3(prompt_q, verbose=False):\n",
    "    if verbose:\n",
    "        print(prompt_q)\n",
    "    response = completion_with_backoff(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt= prompt_q,\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        logprobs=5,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=[\" \", \".\", \",\"]\n",
    "    )\n",
    "    \n",
    "    # default\n",
    "    label_True_False_probs = {\"answer\": False, \"prob_True\": 0.5, \"prob_False\": 0.5}\n",
    "\n",
    "    # Try to get logprobs\n",
    "    logprobs = response[\"choices\"][0][\"logprobs\"] if response and \"logprobs\" in response[\"choices\"][0] else {}\n",
    "    if not logprobs:\n",
    "        print(statement, \"Alert: no logprobs.\")\n",
    "        # No log probs, label_True_False_probs stays as default, no need to process further\n",
    "        return label_True_False_probs\n",
    "\n",
    "    # Process top 5 candidates for next token\n",
    "    candidates = logprobs[\"top_logprobs\"][0]\n",
    "\n",
    "    # Get True score and False score, note that this added up to 80-90% because we only have top 5 logprob\n",
    "    score_true = sum([math.exp(score) for x, score in candidates.items() if boolean_label(x) is True])\n",
    "    score_false = sum([math.exp(score) for x, score in candidates.items() if boolean_label(x) is False])\n",
    "    if verbose:\n",
    "        print(score_true, score_false)\n",
    "\n",
    "    # Get label\n",
    "    max_label = score_true > score_false\n",
    "\n",
    "    # Scale it to 100%\n",
    "    if score_true + score_false != 0.0:\n",
    "        score_true_norm = score_true/(score_false + score_true)\n",
    "        score_false_norm = 1 - score_true_norm\n",
    "        label_True_False_probs = {\"answer\": max_label, \"prob_True\": score_true_norm, \"prob_False\": score_false_norm}\n",
    "    else:\n",
    "        # if true and false not in the top options, label_True_False_probs stays as default\n",
    "        print(statement, \"Alert: true and false not in the top 5 options.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(score_true_norm, score_false_norm)\n",
    "    return label_True_False_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e7251",
   "metadata": {},
   "source": [
    "There are many queries to get model response for, so the following is such that you can choose the 'start' and 'end' indices for which you want to query the API for, and record that in an output file with a 'run_idx'. \n",
    "\n",
    "You may consider making copies of this notebook/code to run the queries in parallel, \n",
    "\n",
    "e.g. run_idx = 0 for start = 0 and end = 976, run_idx = 1 for start = 976 and end = 15976 etc\n",
    "\n",
    "You'd then have output files like:\n",
    "- gpt3_zero-shot_pred_on_full-ET-dataset_0-idx0-976.jsonl\n",
    "- gpt3_zero-shot_pred_on_full-ET-dataset_1-idx976-15976.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1684ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 976/976 [11:07<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# # ask the qns as that for macaw\n",
    "run_idx = 0 \n",
    "start = 0 \n",
    "end = 976\n",
    "with open(\"macaw_query_beaker_input/macaw_query_full-ET-dataset.json\", \"r\") as query_file,\\\n",
    "    open(\"gpt3_query_api_output/gpt3_zero-shot_pred_on_full-ET-dataset_\" + str(run_idx) + \"-idx\" + str(start) + \"-\" + str(end) + \".jsonl\", \"w\") as predfile:\n",
    "    query_lines = query_file.readlines()\n",
    "    for idx, query_line in tqdm(enumerate(query_lines[start:end]), total= len(query_lines[start:end])):\n",
    "        json_query = json.loads(query_line)\n",
    "        #print(json_query)\n",
    "        prompt_q = json_query['question'] + \"\\n\\n\"\n",
    "        #print(prompt_q)\n",
    "        \n",
    "        json_query['gpt3_input_prompt'] = prompt_q\n",
    "        #json_query['gpt3_answer'] = {\"label\": False, \"True\": 0.5, \"False\": 0.5}\n",
    "        json_query['gpt3_answer'] = get_true_false_prob_from_gpt3(prompt_q)\n",
    "        \n",
    "        predfile.write(json.dumps(json_query))\n",
    "        predfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c53b93",
   "metadata": {},
   "source": [
    "The following gives an estimate of the cost for our queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f697dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 words per query (upper bound)\n",
      "40.0 tokens per query (upper bound)\n",
      "4341120.0 total tokens across all 108528 queries\n",
      "\n",
      "Pricing $0.0200  / 1K tokens\n",
      "$ 86.82 for all 108528 queries\n"
     ]
    }
   ],
   "source": [
    "print(30, \"words per query (upper bound)\")\n",
    "print(round(1000/750 * 30, 2), \"tokens per query (upper bound)\")\n",
    "print(round((1000/750 * 30) * len(query_lines), 2), \"total tokens across all {} queries\".format(len(query_lines)))\n",
    "print(\"\\nPricing $0.0200  / 1K tokens\")\n",
    "print(\"$\", round(((1000/750 * 30) * len(query_lines))/1000 * 0.02, 2), \"for all {} queries\".format(len(query_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70fda16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everyday_devices",
   "language": "python",
   "name": "everyday_devices"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
