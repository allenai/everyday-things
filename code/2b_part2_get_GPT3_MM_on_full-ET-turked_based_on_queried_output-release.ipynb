{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cecc1e",
   "metadata": {},
   "source": [
    "This notebook illustrates how we perform constraint reasoning on top of GPT3's outputs based on device + parts list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da4a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "from csp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5cd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "et2triplets_ann = {}\n",
    "with open(\"enriched_mms/full-ET-dataset.tsv\", \"r\") as dataset:\n",
    "    lines = csv.reader(dataset, delimiter = \"\\t\")\n",
    "    for line_idx, line in enumerate(lines):\n",
    "        # skip header\n",
    "        if line_idx == 0:\n",
    "            continue\n",
    "            \n",
    "        # per MM as in an everyday thing sketched by a turker\n",
    "        et_turker = (line[0], line[1])\n",
    "        if et_turker not in et2triplets_ann:\n",
    "            et2triplets_ann[et_turker] = {\"triplets\": [], \"parts-list\": []}\n",
    "           \n",
    "        # collect list of (triplet_tuple, True_False_label)\n",
    "        triplet = ast.literal_eval(line[2])\n",
    "        annotated_relation = (triplet, line[3])\n",
    "        assert annotated_relation not in et2triplets_ann[et_turker][\"triplets\"]\n",
    "        et2triplets_ann[et_turker][\"triplets\"].append(annotated_relation)\n",
    "        \n",
    "        # also collect a list of unique parts\n",
    "        for part in (triplet[0], triplet[2]):\n",
    "            if part not in et2triplets_ann[et_turker][\"parts-list\"]:\n",
    "                et2triplets_ann[et_turker][\"parts-list\"].append(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3260c",
   "metadata": {},
   "source": [
    "## Process predictions from GPT3 (text-davinci-003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c25830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt3_zero-shot_pred_on_full-ET-dataset_0-idx0-976.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_5-idx60976-75976.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_4-idx45976-60976.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_7-idx90976-105976.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_2-idx15976-30976.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_8-idx105976-end.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_3-idx30976-45976.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_1-idx976-15976.jsonl\n",
      "gpt3_zero-shot_pred_on_full-ET-dataset_6-idx75976-90976.jsonl\n"
     ]
    }
   ],
   "source": [
    "et_triplet_2_probTF = {}\n",
    "gpt3_output_dir = \"gpt3_query_api_output/\"\n",
    "for file in os.listdir(gpt3_output_dir):\n",
    "    if \"test\" in file or file.startswith(\".\"):\n",
    "        continue\n",
    "    with open(gpt3_output_dir + file, \"r\") as predfile:\n",
    "        print(file)\n",
    "        prediction_data = predfile.readlines()\n",
    "        for prediction in prediction_data:\n",
    "            json_pred = json.loads(prediction)\n",
    "            #print(json_pred)\n",
    "\n",
    "            et_triplet_str, original_negated_label = json_pred['id'].rsplit(\"-\",1)\n",
    "            \n",
    "            assert et_triplet_str not in et_triplet_2_probTF\n",
    "            et_triplet_2_probTF[et_triplet_str] = {\"answer\": None, \"prob_True\": 0, \"prob_False\": 0}\n",
    "\n",
    "            # Get answer label\n",
    "            et_triplet_2_probTF[et_triplet_str][\"answer\"] = str(json_pred[\"gpt3_answer\"][\"answer\"]).capitalize()\n",
    "\n",
    "            # Get prob scores\n",
    "            et_triplet_2_probTF[et_triplet_str][\"prob_True\"] = json_pred[\"gpt3_answer\"][\"prob_True\"]\n",
    "            et_triplet_2_probTF[et_triplet_str][\"prob_False\"] = json_pred[\"gpt3_answer\"][\"prob_False\"]\n",
    "\n",
    "\n",
    "            assert  et_triplet_2_probTF[et_triplet_str][\"answer\"] == \"True\" or  et_triplet_2_probTF[et_triplet_str][\"answer\"] == \"False\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af938e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% True tuples: 13722/108528 (12.64)\n"
     ]
    }
   ],
   "source": [
    "true_cnt = 0\n",
    "for triplet_ans in et_triplet_2_probTF:\n",
    "    if et_triplet_2_probTF[triplet_ans]['answer'] == 'True':\n",
    "        true_cnt += 1\n",
    "print(\"% True tuples: {}/{} ({})\".format(true_cnt, len(et_triplet_2_probTF), round((true_cnt/len(et_triplet_2_probTF)) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f797f",
   "metadata": {},
   "source": [
    "The below code is a slight modification of that used for Macaw in Notebook 2a (so you may see some parts of that being reused here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84320c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt3_statements_batch_mode(device, perm):\n",
    "    '''\n",
    "    Input: everyday thing, list of tuples for permutation of list of parts\n",
    "    Output: triplet_ans_conf_lst - contains [triplet,  ans, p_statement]\n",
    "            neg_ans_conf_lst - list of p_neg_statement\n",
    "    '''\n",
    "\n",
    "    triplet_ans_conf_lst = [] # list of list\n",
    "    neg_ans_conf_lst = [] # list\n",
    "    for entry in perm:\n",
    "        for rln in all_relations_lst:\n",
    "            \n",
    "            triplet = (entry[0], rln, entry[1])\n",
    "            et_triplet_str = str((device, triplet))\n",
    "            \n",
    "            if et_triplet_str not in et_triplet_2_probTF:\n",
    "                print(\"Need to query gpt3 online for\", et_triplet_str)\n",
    "#                 statement = triplet2statement(triplet)\n",
    "#                 ans, p_statement, p_neg_statement = get_p_statement_and_p_neg_statement(device, statement)\n",
    "            else:\n",
    "                stored_data = et_triplet_2_probTF[et_triplet_str]\n",
    "                ans = stored_data[\"answer\"]\n",
    "                p_statement = stored_data[\"prob_True\"]\n",
    "                p_neg_statement = stored_data[\"prob_False\"]\n",
    "            \n",
    "            \n",
    "            triplet_ans_conf_lst.append([triplet,  ans, p_statement])\n",
    "            neg_ans_conf_lst.append(p_neg_statement)\n",
    "            \n",
    "    return triplet_ans_conf_lst, neg_ans_conf_lst\n",
    "\n",
    "# query gpt on \"everyday thing\"\n",
    "def run_query_gpt_everyday_thing_batch_mode(device, parts):\n",
    "    # get parts\n",
    "    perm = get_parts_perm(device, parts)\n",
    "    # get judgment\n",
    "    triplet_ans_conf_lst, neg_ans_conf_lst = query_gpt3_statements_batch_mode(device, perm)\n",
    "    triplet_ans_conf_lst_true = get_statements_that_macaw_believesT(triplet_ans_conf_lst) # reuse same function\n",
    "    return triplet_ans_conf_lst, triplet_ans_conf_lst_true, neg_ans_conf_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f8ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet_ans_conf_lst, triplet_ans_conf_lst_true, neg_ans_conf_lst =\\\n",
    "#     run_query_gpt_everyday_thing_batch_mode(\"egg\", ['yolk', 'egg white', 'shell membrane', 'shell', 'air cell'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dffee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet_ans_conf_lst, triplet_ans_conf_lst_true, neg_ans_conf_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e72574",
   "metadata": {},
   "source": [
    "# Part 2: Constraint satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604b7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagine_a_device_with_csp(device, turker, outputs_dir, filter_threshold, parts=[]):\n",
    "\n",
    "    device = device.lower()\n",
    "    tag = \"threshold\" + str(filter_threshold)\n",
    "    \n",
    "    lm_query_dir = outputs_dir + \"LMResponses/\" # dir where you want to save output\n",
    "    wcnf_dir = outputs_dir + \"WCNF_format/\" # dir where you want to save these wcnf for reference\n",
    "    plots_dir = outputs_dir + \"VizPlots/\" # dir where you want to store output files\n",
    "    statements_dir = outputs_dir + \"Props/\"# dir where you save data from this run\n",
    "    all_results_filename = device.replace(\" \", \"-\") + \"_\" + turker + \"_\" + tag\n",
    "    for desired_dir in [outputs_dir, lm_query_dir, wcnf_dir, plots_dir, statements_dir]:\n",
    "        make_sure_dir_exists(desired_dir)\n",
    "    \n",
    "    if all_results_filename + \".pkl\" in os.listdir(statements_dir):\n",
    "         # read\n",
    "        with open(statements_dir + all_results_filename + \".pkl\", 'rb') as f:\n",
    "             all_result_dict = pickle.load(f)\n",
    "        print(\"Read from file ...\", len(all_result_dict[\"gpt3_predictions\"]), \"triplets ...\")\n",
    "    else:\n",
    "        # lm response - do not want to query LM again if the same device has been asked\n",
    "        if device.replace(\" \", \"-\") + \"-\" + turker + \".pkl\" in os.listdir(lm_query_dir):\n",
    "            # read\n",
    "            with open(lm_query_dir + device.replace(\" \", \"-\") + \"-\" + turker + \".pkl\", 'rb') as f:\n",
    "                 triplet_ans_conf_lst, triplet_ans_conf_lst_true, neg_ans_conf_lst = pickle.load(f) \n",
    "        else:\n",
    "            # query gpt\n",
    "            triplet_ans_conf_lst, triplet_ans_conf_lst_true, neg_ans_conf_lst = run_query_gpt_everyday_thing_batch_mode(device, parts)\n",
    "            # save\n",
    "            with open(lm_query_dir + device.replace(\" \", \"-\") + \"-\" + turker + \".pkl\", 'wb') as f:\n",
    "                pickle.dump([triplet_ans_conf_lst, triplet_ans_conf_lst_true, neg_ans_conf_lst], f)\n",
    "\n",
    "        # use maxsat\n",
    "        print(\"Running maxsat ...\", len(triplet_ans_conf_lst), \"triplets...\")\n",
    "        model_believe_true_props, maxsat_selected_props = run_maxsat(device, turker, wcnf_dir, triplet_ans_conf_lst, neg_ans_conf_lst, triplet_ans_conf_lst_true, use_only_model_true_props = False)\n",
    "\n",
    "        print(\"Filtering ...\", len(model_believe_true_props), \"triplets...\", len(maxsat_selected_props), \"triplets...\")\n",
    "        # filter based on confidence\n",
    "        model_believe_true_props_filtered = filter_props(model_believe_true_props, filter_threshold)\n",
    "        maxsat_selected_props_filtered = filter_props(maxsat_selected_props, filter_threshold)\n",
    "\n",
    "        # plot\n",
    "        print(\"Generating visualization ...\", len(model_believe_true_props_filtered), \"believed...\", len(maxsat_selected_props_filtered), \"selected\")\n",
    "        generate_graph_png(device, turker, model_believe_true_props_filtered, plots_dir, \"model_believe_true_\" + tag)\n",
    "        generate_graph_png(device, turker, maxsat_selected_props_filtered, plots_dir, \"maxsat_selected_\" + tag)\n",
    "        believed_selected= [k for k,v in model_believe_true_props_filtered.items() if k in maxsat_selected_props_filtered]\n",
    "        generate_graph_png(device, turker, believed_selected, plots_dir, \"believed_selected_\" + tag)\n",
    "\n",
    "        # save result\n",
    "        all_result_dict = {\"gpt3_predictions\": triplet_ans_conf_lst,\\\n",
    "                        \"gpt3_predictions_believe_true\": triplet_ans_conf_lst_true,\\\n",
    "                        \"model_believe_true_props\": model_believe_true_props,\\\n",
    "                        \"maxsat_selected_props\": maxsat_selected_props,\\\n",
    "                        \"filter_threshold\": filter_threshold,\\\n",
    "                        \"model_believe_true_props_filtered\": model_believe_true_props_filtered,\\\n",
    "                        \"maxsat_selected_props_filtered\": maxsat_selected_props_filtered}\n",
    "\n",
    "        with open(statements_dir + all_results_filename + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(all_result_dict, f)\n",
    "        print()\n",
    "    return all_result_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fdfd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir = \"0_gpt3-text-davinci-003-ImagineADevice-CSP-Viz-full-ET-dataset/\"\n",
    "filter_threshold = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36cb252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_et2triplets_ann = sorted(et2triplets_ann, key=lambda et_turker: len(et2triplets_ann[et_turker]['parts-list']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c530e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mm_idx, et_turker in enumerate(sorted_et2triplets_ann) :\n",
    "    print(et_turker, \"MM #\", mm_idx + 1)\n",
    "    \n",
    "    et, turker = et_turker\n",
    "    parts_list = et2triplets_ann[et_turker]['parts-list']\n",
    "    print(len(parts_list))\n",
    "    all_result_dict = imagine_a_device_with_csp(et, turker, outputs_dir, filter_threshold, parts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9ebde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everyday_devices",
   "language": "python",
   "name": "everyday_devices"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
